{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb08688220941b02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724c2898314d50b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479f5d060f9caa7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import Dataset\n",
    "---\n",
    "**Column Description (Data Source: [Smoking and Drinking Dataset with body signal on Kaggle](https://www.kaggle.com/datasets/sooyoungher/smoking-drinking-dataset/data))**\n",
    "\n",
    "- Sex - male, female\n",
    "- Age - round up to 5 year\n",
    "- Height - round up to 5 cm[cm]\n",
    "- Weight [kg]\n",
    "- Waistline\n",
    "- Sight_left - eyesight(left) the column shows how well the left eye sees, where perfect vision = 1.0\n",
    "- Sight_right - eyesight(right) the column shows how well the right eye sees where perfect vision = 1.0\n",
    "- Hear_left - hearing left, 1(normal), 2(abnormal)\n",
    "- Hear_right - hearing right, 1(normal), 2(abnormal)\n",
    "- SBP - Systolic blood pressure[mmHg]\n",
    "- DBP - Diastolic blood pressure[mmHg]\n",
    "- BLDS - BLDS or FSG(fasting blood glucose)[mg/dL]\n",
    "- Tot_chole - total cholesterol[mg/dL]\n",
    "- HDL_chole - HDL cholesterol[mg/dL] - the only fraction of cholesterol that is called \"good\", \"useful\" cholesterol.\n",
    "- LDL_chole - LDL cholesterol[mg/dL]\n",
    "- Triglyceride - triglyceride[mg/dL]\n",
    "- Hemoglobin - hemoglobin[g/dL]\n",
    "- Urine_protein - protein in urine, 1(-), 2(+/-), 3(+1), 4(+2), 5(+3), 6(+4)\n",
    "- Serum_creatinine - serum(blood) creatinine[mg/dL]\n",
    "- SGOT_AST - SGOT(Glutamate-oxaloacetate transaminase) AST(Aspartate transaminase)[IU/L]\n",
    "- SGOT_ALT - ALT(Alanine transaminase)[IU/L]\n",
    "- Gamma_GTP - y-glutamyl transpeptidase[IU/L]\n",
    "- SMK_stat_type_cd - Smoking state, 1(never), 2(used to smoke but quit), 3(still smoke)\n",
    "- DRK_YN - Drinker or Not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805d021",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def percentage_missing_values(data):\n",
    "    total_cells = np.product(data.shape)\n",
    "\n",
    "    missing_cells = data.isnull().sum().sum()\n",
    "\n",
    "    percentage_missing = (missing_cells / total_cells) * 100\n",
    "    return f\"Percentage of missing values: {percentage_missing:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfeb1e3f35d16c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sd.csv')\n",
    "df.drop('SMK_stat_type_cd',axis=1,inplace=True)\n",
    "df_copy = df.copy()\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ff308",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "percentage_missing_values(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ce6ac156f3cc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Useful functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cee908add2d907",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers_from_columns(df, column_names):\n",
    "\n",
    "    for column_name in column_names:\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        Q1 = df[column_name].quantile(0.25)\n",
    "        Q3 = df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter out outliers\n",
    "        df = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7abcc13a93e2c1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def percentage_missing_values(data):\n",
    "    total_cells = np.product(data.shape)\n",
    "\n",
    "    missing_cells = data.isnull().sum().sum()\n",
    "\n",
    "    percentage_missing = (missing_cells / total_cells) * 100\n",
    "    return f\"Percentage of missing values: {percentage_missing:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89898db7d78630fe",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def duplicate_data(data):\n",
    "\n",
    "    duplicates_cells = data[data.duplicated].shape\n",
    "    \n",
    "    return f\"Number of duplicates: {duplicates_cells[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469278eea16bc70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clean data\n",
    "---\n",
    "- Remove outliers\n",
    "- Remove duplicates\n",
    "- impute missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d1a62ddf0b8ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Remove Outliers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa1f8b0e1ae50",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40e0e5da80beb8",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_copy = remove_outliers_from_columns(df_copy, ['waistline','SBP', 'DBP','BLDS','tot_chole','triglyceride','serum_creatinine','SGOT_AST', 'SGOT_ALT','sight_left','sight_right','gamma_GTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b80719e1c2a5a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50189ec4d3f6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Remove Duplicates\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117fe644664da0d",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "duplicate_data(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = df_copy.drop_duplicates(keep= 'last')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "be53f201066bcc89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicate_data(df_copy)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7f796b2397584faa"
  },
  {
   "cell_type": "markdown",
   "id": "1d486ba5aa6421fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Impute Missing Values \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39131a55ca7c6ab1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "percentage_missing_values(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada96391d295c6d5",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\"print(df_copy.select_dtypes(include=[object,bool]).columns)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c5c40",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\"df_copy.info()\"\"\"\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1efb708d9f8a5c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_analysis = df_copy.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "71678e722413f2d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantitative_columns = ['age', 'height', 'weight', 'waistline', 'sight_left', 'sight_right','SBP', 'DBP', 'BLDS', 'tot_chole', 'HDL_chole', 'LDL_chole', 'triglyceride', 'hemoglobin', 'serum_creatinine', 'SGOT_AST','SGOT_ALT', 'gamma_GTP']\n",
    "data_quantitative = d_analysis[quantitative_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "383fc48a9737c11d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_analysis['hear_left'] = d_analysis['hear_left'].map({1: 'Normal', 2: 'Abnormal'})\n",
    "d_analysis['hear_right'] = d_analysis['hear_right'].map({1: 'Normal', 2: 'Abnormal'})\n",
    "d_analysis['urine_protein'] = d_analysis['urine_protein'].map({1: '-', 2: '+/-', 3: '+1', 4: '+2', 5: '+3', 6: '+4'})"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d70d75a313b750cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))  # You can adjust the size of the plot as needed\n",
    "correlation_matrix = data_quantitative.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Numerical Values')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6dd891666493d7f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "class DataAnalysis:\n",
    "    def __init__(self, data):\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"Data must be a pandas DataFrame\")\n",
    "        self.data = data\n",
    "\n",
    "    def univariate_analysis(self, column):\n",
    "        print(f\"Univariate Analysis for {column}:\")\n",
    "        data = self.data[column]\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(data) and not pd.api.types.is_bool_dtype(data):\n",
    "            # Summary statistics for numerical data\n",
    "            summary_stats = data.describe()\n",
    "            print(\"Summary Statistics:\")\n",
    "            print(summary_stats)\n",
    "\n",
    "            # Distribution plot\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.histplot(data, kde=True)\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.show()\n",
    "\n",
    "            # Box plot\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.boxplot(x=data)\n",
    "            plt.title(f'Box Plot of {column}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Summary statistics for categorical or boolean data\n",
    "            value_counts = data.value_counts()\n",
    "            print(\"Value Counts:\")\n",
    "            print(value_counts)\n",
    "\n",
    "            # Bar plot\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.countplot(y=data, order = data.value_counts().index)\n",
    "            plt.title(f'Bar Plot of {column}')\n",
    "            plt.show()\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    def bivariate_analysis(self, column1, column2):\n",
    "        print(f\"Bivariate Analysis for {column1} and {column2}:\")\n",
    "        data1 = self.data[column1]\n",
    "        data2 = self.data[column2]\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(data1) and pd.api.types.is_numeric_dtype(data2) and not pd.api.types.is_bool_dtype(data1) and not pd.api.types.is_bool_dtype(data2):\n",
    "            # Pearson correlation for numerical data\n",
    "            correlation, p_value = pearsonr(data1, data2)\n",
    "            print(f\"Pearson Correlation: {correlation}, P-value: {p_value}\")\n",
    "            \n",
    "        else:\n",
    "            # ANOVA test for numerical vs categorical\n",
    "            if pd.api.types.is_numeric_dtype(data1) and not pd.api.types.is_bool_dtype(data1):\n",
    "                model = ols(f'{column1} ~ C({column2})', data=self.data).fit()\n",
    "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "                print(\"ANOVA Test Results:\")\n",
    "                print(anova_table)\n",
    "            elif pd.api.types.is_numeric_dtype(data2) and not pd.api.types.is_bool_dtype(data2):\n",
    "                model = ols(f'{column2} ~ C({column1})', data=self.data).fit()\n",
    "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "                print(\"ANOVA Test Results:\")\n",
    "                print(anova_table)\n",
    "\n",
    "            # Chi-squared test for categorical vs categorical\n",
    "            if (pd.api.types.is_categorical_dtype(data1) or pd.api.types.is_bool_dtype(data1)) and (pd.api.types.is_categorical_dtype(data2) or pd.api.types.is_bool_dtype(data2)):\n",
    "                contingency_table = pd.crosstab(data1, data2)\n",
    "                chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "                print(f\"Chi-squared Test Results: Chi2 = {chi2}, P-value = {p}, Degrees of Freedom = {dof}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    def full_analysis(self):\n",
    "        columns = self.data.columns.tolist()\n",
    "\n",
    "        if not columns:\n",
    "            print(\"No columns found for analysis.\")\n",
    "            return\n",
    "\n",
    "        # Univariate Analysis\n",
    "        print(\"Performing Univariate Analysis...\\n\")\n",
    "        for column in columns:\n",
    "            self.univariate_analysis(column)\n",
    "\n",
    "        # Bivariate Analysis\n",
    "        print(\"Performing Bivariate Analysis...\\n\")\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i+1, len(columns)):\n",
    "                self.bivariate_analysis(columns[i], columns[j])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9c33501b5eaecfeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assume df is your pandas DataFrame\n",
    "analysis = DataAnalysis(d_analysis)\n",
    "analysis.full_analysis()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fac25412a2afd6e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy.replace({'DRK_YN':{'Y':1, 'N':0}}, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b160689072d4317c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling with One Hot Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b74b8f0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, r2_score, mean_squared_error,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "27aa8ccb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categ = df_copy.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "num = df_copy.select_dtypes(exclude=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "num.remove('DRK_YN')\n",
    "preprocessor = make_column_transformer(\n",
    "        (OneHotEncoder(), categ),\n",
    "        (StandardScaler(), num)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "067bea91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classifier Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c4259b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00ca91",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier_metrics(classifier, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    test_predictions = classifier.predict(X_test)\n",
    "    train_predictions = classifier.predict(X_train)\n",
    "    print('Train Set Classification Results:')\n",
    "    print(classification_report(y_train,train_predictions))\n",
    "    print('\\nTest Set Classification Results:')\n",
    "    print(classification_report(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab008e4",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed64c7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df_copy.drop('DRK_YN',axis=1)\n",
    "y = df_copy['DRK_YN'] \n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "knn_metrics = evaluate_classifier_metrics(knn_classifier, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce88f28",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c9a93",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "logistic_classifier = LogisticRegression()\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "logistic_metrics = evaluate_classifier_metrics(logistic_classifier, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b7dc50e42ebda9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5075fe0d2aac4a64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if fanalysis is already installed, import it - else install and import it\n",
    "try:\n",
    "    from fanalysis.pca import PCA\n",
    "except:\n",
    "    !pip install fanalysis\n",
    "    from fanalysis.pca import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "23c04f36dbfda6c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA Copy\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c343c886856a4c48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_copy = df_copy.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9a9107eb84585a3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Variable Mappings\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d519c23b80ce6ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard Scaling\n",
    "---\n",
    "- PCA works by identifying the directions (principal components) in which the data varies the most. If the features are on different scales, the features with higher magnitude (regardless of their actual variance in the data) will dominate the first principal components, potentially leading to misleading results. Standardizing ensures that all features contribute equally to the calculation of the principal components.\n",
    "- PCA is sensitive to the mean of the data. By subtracting the mean of each feature from the data, standard scaling centers the data around the origin. This is important because PCA identifies the principal components using covariance or correlation, both of which are affected by the mean of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "831495d84913eb6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "quantitative_columns = ['age', 'height', 'weight', 'waistline', 'sight_left', 'sight_right','SBP', 'DBP', 'BLDS', 'tot_chole', 'HDL_chole', 'LDL_chole', 'triglyceride', 'hemoglobin', 'serum_creatinine', 'SGOT_AST','SGOT_ALT', 'gamma_GTP']\n",
    "data_quantitative = pca_copy[quantitative_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data_quantitative)\n",
    "\n",
    "D = pd.DataFrame(data = data_standardized, columns = quantitative_columns)\n",
    "# define dimensions of [X] matrix : number of variables\n",
    "p = D.shape[1]\n",
    "\n",
    "# define sample size of [X] : number of observations\n",
    "n = D.shape[0]\n",
    "\n",
    "# define values of matrix [X]\n",
    "X = D.values\n",
    "\n",
    "print(D.head())\n",
    "# print dataset summary\n",
    "print(D.info())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "13319751526e938c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instantiate acp object form PCA class\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75a3add25be66913"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acp = PCA(std_unit=True,row_labels=D.index,col_labels=D.columns)\n",
    "\n",
    "# run PCA on X observed data\n",
    "acp.fit(X)\n",
    "\n",
    "# print methods and attributes of acp object\n",
    "dir(acp)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8b896b62d9d0a5a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acp.col_labels"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "932464be1e987935"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acp.row_labels"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "420e80f83ac8a3eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Determine the number of factors\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa3ab13c19734e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print eigenvalues (the lambdas) - output includes proportion of variance explained and cumulated values thereof\n",
    "print(acp.eig_)\n",
    "\n",
    "# row 1 : lambda 1, lambda 2, ..., lambda k\n",
    "# IP is the sum of lambda\n",
    "# row 2 : lambda 1 / IP, lambda 2 / IP, ..., lambda k/ IP\n",
    "# Cumulative "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a7234c739ffa3127"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print scree plot\n",
    "\n",
    "# first the main plot\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(range(1,p+1),acp.eig_[0],\".-\")\n",
    "ax.set_xlabel(\"Nb. of factors\")\n",
    "ax.set_ylabel(\"Eigenvalues\")\n",
    "plt.title(\"Scree Plot\")\n",
    "\n",
    "# add Kaiser's threshold line\n",
    "ax.plot([1,p],[1,1],\"r--\",linewidth=1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a4c80df2fe50be3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dimensionality Reduction from 18 to 6 using PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a widely used technique for dimensionality reduction in data analysis and machine learning. It transforms the original variables to a new set of variables, the principal components, which are orthogonal (at right angles to each other), and which reflect the maximum variance in the data.\n",
    "\n",
    "## The Kaiser-Guttman Rule\n",
    "\n",
    "The Kaiser-Guttman rule is a popular criterion for selecting the number of principal components to retain in a PCA. According to this rule, we should only keep the components with eigenvalues greater than 1.\n",
    "\n",
    "In our specific case, applying the Kaiser-Guttman rule suggests that we should retain 6 principal components.\n",
    "\n",
    "## Importance of Reducing from 18 to 6 Dimensions\n",
    "\n",
    "In conclusion, reducing the dimensionality of the dataset from 18 to 6 using PCA, guided by the Kaiser-Guttman rule, not only simplifies the dataset but also enhances the efficiency and performance of subsequent analyses and models, while potentially improving our understanding of the underlying patterns in the data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "718cd517fa043774"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print explained variance plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(range(0,p+1),np.append(0,acp.eig_[2]),\".-\")\n",
    "ax.set_xlabel(\"Nb. of factors\")\n",
    "ax.set_ylabel(\"% of explained variance\")\n",
    "plt.title(\"Explained Variance\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9ac97110a68c0c7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'\\nExplained Variance by the first 6 components: {sum(acp.eig_[1][:6]):.2f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "249495a3caf4623f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying Barlett's test of Sphericity\n",
    "# import statistical package from scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Bartlett's statistic\n",
    "C = -(n-1-(2*p+5)/6) * np.sum(np.log(acp.eig_[0]))\n",
    "print(f\"Barlett's statistics: {C:.3f}\")\n",
    "\n",
    "# degree of freedom\n",
    "ddl = p*(p-1)/2\n",
    "\n",
    "# p-value\n",
    "print(f\"p-value: {1-stats.chi2.cdf(C,df=ddl):.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2811843925fc4b02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bartlett’s Test of Sphericity Results\n",
    "\n",
    "#### Interpretation:\n",
    "- **Bartlett’s Test Statistic:** 6,857,070.377\n",
    "- This is a large value, indicating strong evidence against the null hypothesis.\n",
    "\n",
    "- **P-value:** 0.000\n",
    "- A p-value of 0.000 (which might be rounded and actually be a very small number, but not exactly zero) indicates that the probability of observing a test statistic as extreme as this, assuming the null hypothesis is true, is virtually zero.\n",
    "\n",
    "#### Conclusion:\n",
    "Since the p-value is extremely low, we reject the null hypothesis of Bartlett’s test of sphericity. This suggests that there is significant evidence in the data to indicate that the variables are correlated. This is a positive indication if you are planning to proceed with factor analysis or PCA, as these techniques assume that there are relationships between the variables that can be captured by underlying factors or principal components. In other words, the test result suggests that your data is likely suitable for these types of dimensionality reduction techniques.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34675f25a293d7b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variables Representation\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c21c4ca857911b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correlation between variables and factors\n",
    "print(pd.DataFrame(acp.col_coord_[:,:6],index=D.columns, columns=['F1','F2','F3','F4','F5','F6']))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a643e9062add481e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Contributions of each variable on the two first factors (in %)\n",
    "print(pd.DataFrame(acp.col_contrib_[:,:6],index=D.columns, columns=['F1','F2','F3','F4','F5','F6']))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4e35ff7a5ee54018"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Factor 1 (F1)\n",
    "### High Loadings:\n",
    "- Age\n",
    "- Weight\n",
    "- Waistline\n",
    "- Hemoglobin\n",
    "- Serum Creatinine\n",
    "- SGOT_ALT\n",
    "- Gamma_GTP\n",
    "\n",
    "### Interpretation:\n",
    "This factor might represent overall metabolic and aging processes, as it has high loadings on age, weight, and waistline (possibly indicating body size or obesity), as well as hemoglobin and serum creatinine (which are related to kidney function and blood). The liver enzymes (SGOT_ALT and gamma_GTP) also have high loadings, suggesting a possible link to liver function. This factor could be interpreted as a 'metabolic and aging' factor.\n",
    "\n",
    "## Factor 2 (F2)\n",
    "### High Loadings:\n",
    "- Sight Left\n",
    "- Sight Right\n",
    "- Systolic Blood Pressure (SBP)\n",
    "- Diastolic Blood Pressure (DBP)\n",
    "\n",
    "### Interpretation:\n",
    "This factor has high loadings on both left and right sight measurements, as well as systolic and diastolic blood pressure (SBP and DBP). This could indicate a factor related to cardiovascular and eye health, possibly capturing aspects of blood circulation and its effects on vision. This factor might be interpreted as a 'cardiovascular and vision' factor.\n",
    "\n",
    "## Factor 3 (F3)\n",
    "### High Loadings:\n",
    "- Total Cholesterol (Tot_Chole)\n",
    "- LDL Cholesterol (LDL_Chole)\n",
    "\n",
    "### Interpretation:\n",
    "This factor has high loadings on total cholesterol and LDL cholesterol, which are key indicators of lipid metabolism and cardiovascular health. This factor might represent lipid metabolism and cardiovascular risk, as higher levels of LDL cholesterol are typically associated with an increased risk of heart disease.\n",
    "\n",
    "## Factor 4 (F4)\n",
    "### High Loadings:\n",
    "- Systolic Blood Pressure (SBP)\n",
    "- Diastolic Blood Pressure (DBP)\n",
    "- SGOT_AST (A Liver Enzyme)\n",
    "\n",
    "### Interpretation:\n",
    "This factor has high loadings on systolic and diastolic blood pressure, as well as SGOT_AST (a liver enzyme). The combination of high blood pressure and liver enzyme levels could indicate a factor related to cardiovascular and liver health, possibly capturing the effects of hypertension on the body, including potential impacts on liver function.\n",
    "\n",
    "## Factor 5 (F5)\n",
    "### High Loadings:\n",
    "- HDL Cholesterol (HDL_Chole)\n",
    "- Triglyceride\n",
    "- SGOT_AST (A Liver Enzyme)\n",
    "\n",
    "### Interpretation:\n",
    "This factor has high loadings on HDL cholesterol (often considered “good” cholesterol), triglycerides, and SGOT_AST. This might represent a lipid and liver function factor, capturing aspects of lipid metabolism and its relationship with liver health. Higher levels of HDL cholesterol are generally considered protective against heart disease, while higher triglycerides can be a risk factor.\n",
    "\n",
    "## Factor 6 (F6)\n",
    "### High Loadings:\n",
    "- Sight Left\n",
    "- Sight Right\n",
    "- HDL Cholesterol (HDL_Chole)\n",
    "\n",
    "### Interpretation:\n",
    "This factor has high loadings on both left and right sight measurements, as well as HDL cholesterol. This could indicate a factor related to vision and cardiovascular health, possibly capturing the relationship between good cholesterol levels and eye health.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b289c0eeaa44d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
